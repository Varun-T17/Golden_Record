{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data proessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Process - Emma@ Johnson Capital12 -> emma johnson capital12\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_name(name):\n",
    "    #convert to lower case\n",
    "    name = name.lower()\n",
    "    #remove non alphanumeric characters\n",
    "    name = re.sub('[^A-Za-z0-9]+', ' ', name)\n",
    "    #remove spaces from beginning and end of the string\n",
    "    name = name.strip()\n",
    "    #convert consecutive spaces/tabs/newlines in a single space\n",
    "    name = \" \".join(name.split())\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process company names - HSBC & Holdings 12 ltd. -> hsbc 12\n",
    "\n",
    "import re\n",
    "\n",
    "def company_processor(name):\n",
    "    #convert to lower case\n",
    "    name = name.lower()\n",
    "    #remove spaces from beginning and end of the string\n",
    "    name = name.strip()\n",
    "    #convert consecutive spaces/tabs/newlines in a single space\n",
    "    name = \" \".join(name.split())\n",
    "    #convert limited (with any combination of .) to ltd and company/co. (with any combination of .) to co\n",
    "    name = re.sub(r'limited', r'ltd', name)\n",
    "    name = re.sub(r'l\\.*t\\.*d\\.*', r'ltd', name)\n",
    "    name = re.sub(r'company', r'co', name)\n",
    "    name = re.sub(r'c\\.*o.\\*', r'co', name)\n",
    "    #Countries\n",
    "    #convert & to \"and\"\n",
    "    name = name.replace('&', ' and ')\n",
    "\n",
    "    text = name \n",
    "\n",
    "    #convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    stop_words = ['ltd', 'limited', 'holdings' ,'plc', 'of', \"sme\", 'and']\n",
    "   #\"co\", \"-\"\n",
    "   \n",
    "    # Split the string into a list of words \n",
    "    words = text.split() \n",
    "\n",
    "    # Create a new list to hold the filtered words \n",
    "    filtered_words =  \"\" \n",
    "\n",
    "    # Iterate over the list of words \n",
    "    for word in words: \n",
    "        # If the word is not in the stop word list, add it to the filtered list \n",
    "        if word not in stop_words: \n",
    "            filtered_words = filtered_words + \" \" + word\n",
    "    filtered_words = filtered_words.strip()       \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Long & Lat of given location through API and find distance between them\n",
    "\n",
    "import geopy as geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(timeout=10, user_agent=\"Krukarius\")\n",
    "\n",
    "def find_location(place):\n",
    "    location = geolocator.geocode(place)\n",
    "    location_overall = geolocator.geocode(location)\n",
    "    return location_overall.latitude, location_overall.longitude\n",
    "\n",
    "import haversine as hs\n",
    "\n",
    "def Distance(loc1,loc2):\n",
    "    return hs.haversine(loc1,loc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve username from email address, and splits if there is a .\n",
    "\n",
    "def extract_username(email):\n",
    "    keywordsPhrase = \"\"\n",
    "\n",
    "    if pd.isnull(email):\n",
    "        keywordsPhrase = \"\"\n",
    "    else:\n",
    "        # Remove any leading or trailing spaces\n",
    "        email = email.strip()\n",
    "\n",
    "        # Split the email address into two parts: the local part and the domain part\n",
    "        local_part, domain_part = email.split('@')\n",
    "\n",
    "        # Remove any dots from the local part\n",
    "        local_part = local_part.replace('.', '_')\n",
    "        local_part = re.sub(r'\\d+', '_', local_part)\n",
    "\n",
    "        # Extract the keywords from the local part\n",
    "        keywords = local_part.split('_')\n",
    "\n",
    "        keywordsList = list(filter(lambda x: x.strip() != \"\", keywords))\n",
    "        for string in keywordsList:\n",
    "            keywordsPhrase += f\"{string} \"\n",
    "\n",
    "    return keywordsPhrase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-Layer Matching Process using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a CSR Matrix. An efficient representation of sparse (many zeros) matrix*\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, triu\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # CSR matrix -- efficient representation of sparse (many zeros) matrix\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    # rows of A\n",
    "    M, _ = A.shape\n",
    "    # columns of B\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "# Efficient dot product and selection of ntop most-similar vectors\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matches given matrix and dataframe\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_matches_SF_brokers(sparse_matrix, df):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    nr_matches = sparsecols.size\n",
    "    \n",
    "    left_row_ids = np.empty([nr_matches], dtype=object)\n",
    "    right_row_ids = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    row_ids = df['ID'].values\n",
    "    \n",
    "    for i in range(0, nr_matches):\n",
    "        left_row_ids[i] = row_ids[sparserows[i]]\n",
    "        right_row_ids[i] = row_ids[sparsecols[i]]\n",
    "        similairity[i] = sparse_matrix.data[i]\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame({'ID': left_row_ids,\n",
    "                         'SIMILARITY': similairity,\n",
    "                         'MATCHED_ID': right_row_ids})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data\n",
    "\n",
    "The data is from https://www.briandunning.com/sample-data/.\n",
    "I have removed some fields in some rows in order to test accuracy of this program when we have incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Data = pd.read_csv('SampleData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address_generator concatenates information from each column for searching adress's long & lat\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# processor() returns nothing if empty, else returns string\n",
    "\n",
    "def processor(input):\n",
    "    pattern = r'\\b\\w+\\b'\n",
    "\n",
    "    if pd.isnull(input):\n",
    "        input = \"\"\n",
    "    else:\n",
    "        # Find all matches using the pattern\n",
    "        matches = re.findall(pattern, input)\n",
    "\n",
    "        # Convert matches to lowercase and join with a single space in between\n",
    "        input = ' '.join(match.lower() for match in matches)\n",
    "\n",
    "    return input\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "def address_generator(row):\n",
    "    column1_value = processor(row['address'])\n",
    "    column2_value = processor(row['city'])\n",
    "    column3_value = processor(row['county'])\n",
    "    column4_value = processor(row['postal'])\n",
    "\n",
    "    address = f\"{column1_value} {column2_value} {column3_value} {column4_value}\"\n",
    "    address = address.strip()\n",
    "    address = ' '.join(address.split())\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates name from first name and last name columns\n",
    "def name_generator(row):\n",
    "    column1_value = processor(row['first_name'])\n",
    "    column2_value = processor(row['last_name'])\n",
    "\n",
    "    name = f\"{column1_value} {column2_value}\"\n",
    "    name = name.strip()\n",
    "    name = ' '.join(name.split())\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins all company, name, email and address information together from all processed sources\n",
    "\n",
    "def complete_stopwords(row):\n",
    "    column1_value = row['company_name_stopwords']\n",
    "    column2_value = row['address_name_stopwords']\n",
    "    column3_value = row['email_stopwords']\n",
    "    column4_value = row['name_stopwords']\n",
    "\n",
    "    output = f\"{column1_value} {column2_value} {column3_value} {column4_value}\"\n",
    "    output = output.strip()\n",
    "    output = ' '.join(output.split())\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organising processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Company Name Data\n",
    "Data[\"company_key_words\"] = Data[\"company_name\"].apply(lambda x: company_processor(str(x)))\n",
    "Data[\"address_string\"] = Data.apply(lambda row: address_generator(row), axis=1)\n",
    "Data[\"email_username\"] = Data[\"email\"].apply(lambda x: extract_username(x))\n",
    "# Data[\"Co-ordinates\"] = Data[\"address_name_stopwords\"].apply(lambda x: find_location(x))\n",
    "Data[\"name_stopwords\"] = Data.apply(lambda row: preprocess_name(row), axis=1)\n",
    "Data[\"all stopwords\"] = Data.apply(lambda row: complete_stopwords(row), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELFTIMED: 0.00700688362121582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "tf_idf_matrix = vectorizer.fit_transform(Data['all stopwords'])\n",
    "\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 100000, 0.2)\n",
    "\n",
    "tri_upper_no_diag = triu(matches, k=1)\n",
    "\n",
    "matches_df = get_matches_SF_brokers(tri_upper_no_diag, Data)\n",
    "\n",
    "matches_df = pd.merge(matches_df, Data, how='inner', left_on= 'ID' , right_on = 'ID', suffixes = ('', '_1'))\n",
    "matches_df = pd.merge(matches_df, Data, how='inner', left_on= 'MATCHED_ID' , right_on = 'ID', suffixes = ('', '_MATCHED'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second-Layer of checking between matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/v.tiwari/Library/Python/3.9/lib/python/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Generate score for matching between fields of same columns\n",
    "\n",
    "# For names, check simialrity of sound.\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def Similar_Sound_Of_Name(a, b):\n",
    "    Similarity1 = 0\n",
    "    if pd.notnull(a) and pd.notnull(b):\n",
    "        Similarity1 = (fuzz.ratio(a, b))/200\n",
    "    else:\n",
    "        Similarity1 = 0.25\n",
    "    return Similarity1\n",
    "\n",
    "def discLeven(a, b):\n",
    "    return abd.DiscountedLevenshtein().sim(a,b)\n",
    "\n",
    "# For company name, check difference in letters\n",
    "def company_name_similarity(x, y):\n",
    "    if pd.notnull(x) and pd.notnull(y):\n",
    "        return discLeven(x,y)\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "# For email username, check difference in letters\n",
    "def email_similarity(x, y):\n",
    "    x.replace(\" \", \"\")\n",
    "    y.replace(\" \", \"\")\n",
    "    z = 0\n",
    "    if pd.notnull(x) and pd.notnull(y):\n",
    "        z =  discLeven(x,y)\n",
    "    else:\n",
    "        z = 0.5\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give score for matches using process above\n",
    "\n",
    "matches_df['Name Similarity'] = matches_df.apply(lambda x: (Similar_Sound_Of_Name(x['first_name'],x['first_name_MATCHED']) + Similar_Sound_Of_Name(x['last_name'],x['last_name_MATCHED'])), axis=1)\n",
    "matches_df['Company Name Similarity'] = matches_df.apply(lambda x: company_name_similarity(x['company_name_stopwords'],x['company_name_stopwords_MATCHED']),axis=1)\n",
    "# matches_df['Location Closeness'] = matches_df.apply(lambda x: hs.haversine(x['Co-ordinates'],x['Co-ordinates_MATCHED']),axis=1)\n",
    "matches_df['Email Similarity'] = matches_df.apply(lambda x: email_similarity(x['email_stopwords'],x['email_stopwords_MATCHED']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID  SIMILARITY          MATCHED_ID\n",
      "0   0018d000006ytVSCLR    0.793270  0018d000006ytVSAXX\n",
      "1   0018d000006ytVSALT    0.833333  0018d000006ytVSAXF\n",
      "2   0018d000006ytVSAWS    0.480321  0018d000006ytVSAXF\n",
      "3   0018d000006ytVSALT    0.396988  0018d000006ytVSAWS\n",
      "4   0018d000006ytVSAAV    0.526176  0018d000006ytVSAWS\n",
      "..                 ...         ...                 ...\n",
      "77  0018d000006ytVSAVF    1.000000  0018d000006ytVSAWZ\n",
      "78  0018d000006ytVSAVJ    0.860314  0018d000006ytVSAWY\n",
      "79  0018d000006ytVSAVP    0.638638  0018d000006ytVSAWT\n",
      "80  0018d000006ytVSAVV    0.966618  0018d000006ytVSAWX\n",
      "81  0018d000006ytVSAWH    0.871747  0018d000006ytVSAWW\n",
      "\n",
      "[78 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average of similarity and implement a threshold to define matches\n",
    "\n",
    "import statistics\n",
    "\n",
    "# matches_df['SIMILARITY'] = matches_df.apply(lambda x: statistics.fmean(x[['Name Similarity','Company Name Similarity','Location Closeness', 'Email Similarity']]),axis=1)\n",
    "matches_df['SIMILARITY'] = matches_df.apply(lambda x: statistics.fmean(x[['Name Similarity','Company Name Similarity', 'Email Similarity']]),axis=1)\n",
    "salesforce_Similarity = matches_df.loc[matches_df['SIMILARITY'] >= 0.2]\n",
    "out = salesforce_Similarity[['ID', 'SIMILARITY', 'MATCHED_ID']]\n",
    "out.to_csv('SampleData_output.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating groups of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a graph, use matches as edges and IDs as Nodes. Give MDM_ID to similar records.\n",
    "\n",
    "import networkx\n",
    "\n",
    "sf_brokers = pd.read_csv('SampleData.csv')\n",
    "sf_matches = pd.read_csv('SampleData_output.csv')\n",
    "sf_brokers = sf_brokers.replace(np.nan, '', regex=True)\n",
    "# create all groups \n",
    "from networkx import connected_components, draw\n",
    "\n",
    "nodelist = sf_brokers['ID'].values\n",
    "edgelist = [(x,y) for x,y in zip(sf_matches['ID'].values, sf_matches['MATCHED_ID'].values) if y != 0]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodelist)\n",
    "G.add_edges_from(edgelist)\n",
    "cc = connected_components(G)\n",
    "\n",
    "tp = []\n",
    "i = 0\n",
    "for x in list(cc):\n",
    "    i += 1\n",
    "    for r in x:\n",
    "        tp.append((r, 'SF_BR_'+str(i),i))\n",
    "\n",
    "df = pd.DataFrame.from_records(tp, columns=['ID', 'MDM_ID','MDM_ID_NUM'])\n",
    "sf_brokers_match = pd.merge(sf_brokers, df, on='ID')\n",
    "sf_brokers_match.to_csv('SampleData_Groups.csv', index = False)\n",
    "\n",
    "sf_brokers_match.sort_values('MDM_ID_NUM', inplace=True, ascending=True)\n",
    "sf_brokers_match.drop('MDM_ID_NUM', axis=1)\n",
    "sf_brokers_match.to_excel('SampleData_groups.xlsx', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Golden Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 11, 12, 32, 38, 39, 61, 64, 76, 77, 90, 93, 96, 119, 121, 145, 149, 151, 162, 170, 184, 192, 202, 204, 207, 212, 233, 238, 240, 252, 258, 262, 267, 268, 271, 274, 280, 285, 302, 307, 313, 319, 321, 328, 329, 363, 369, 374, 380, 385, 388, 401, 420, 424, 426, 433, 444, 448, 454, 459, 471]\n"
     ]
    }
   ],
   "source": [
    "# Lists all non unique Records\n",
    "\n",
    "data_list = sf_brokers_match['MDM_ID_NUM']\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(data_list)\n",
    "non_unique_values = [value for value, count in counter.items() if count > 1]\n",
    "print(non_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 11, 12, 32, 38, 39, 61, 64, 76, 77, 90, 93, 96, 119, 121, 145, 149, 151, 162, 170, 184, 192, 202, 204, 207, 212, 233, 238, 240, 252, 258, 262, 267, 268, 271, 274, 280, 285, 302, 307, 313, 319, 321, 328, 329, 363, 369, 374, 380, 385, 388, 401, 420, 424, 426, 433, 444, 448, 454, 459, 471]\n",
      "here is the data:\n",
      "Empty DataFrame\n",
      "Columns: [ID, MDM_ID, MDM_ID_NUM]\n",
      "Index: []\n",
      "which values would you like to group together, separate by commas\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z4/678bgh8x4l7f379pnmq4g2bh0000gp/T/ipykernel_5737/3328132838.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"which values would you like to group together, separate by commas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mAnswer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mAnswer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnswer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mresult_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnswer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mSpecific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3944\u001b[0m         for the SettingWithCopyWarning).\n\u001b[1;32m   3945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3946\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3947\u001b[0m         \"\"\"\n\u001b[0;32m-> 3948\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3949\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3951\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3920\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3921\u001b[0m         \"\"\"\n\u001b[1;32m   3922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             if (\n\u001b[1;32m   3925\u001b[0m                 \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3926\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# Under Construction - Ask which records to keep which to remove and so on\n",
    "\n",
    "Answer = \"\"\n",
    "while Answer != \"N\":\n",
    "    print(non_unique_values)\n",
    "    hello = input(\"here are the non unique ids, which would you like?, if you want to end press N\")\n",
    "    print(\"here is the data:\")\n",
    "    result = df[df['MDM_ID_NUM'] == Answer]\n",
    "    print(result)\n",
    "    print(\"which values would you like to group together, separate by commas\")\n",
    "    Answer2 = input()\n",
    "    Answer2 = Answer2.replace(\" \", \"\")\n",
    "    result_set = list(Answer2.split(','))\n",
    "    Specific = df.iloc[result_set]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
